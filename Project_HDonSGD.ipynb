{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_\n",
        "\n",
        "## Instructions\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIeFXS0F0zww",
        "outputId": "638cff88-560c-495b-e3b3-7deb87b66d18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.6.0\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools Plots\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -n \"$COLAB_GPU\" ] && [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  if [ \"$COLAB_GPU\" = \"1\" ]; then\n",
        "      JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &>/dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Success! Please reload this page and jump to the next section.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjYzDaZFepN2"
      },
      "source": [
        "### **[An implementation of Hypergradient Descent on Stocastic gradient Descent]**\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "### **ARGS:**\n",
        "\n",
        "\n",
        "### **x** : will be our initial guess for finding the optimum.\n",
        "### **2x-2** : derivate of the function we our trying to find optimum, so basically gradient for univariate function.\n",
        "### **learn_rate** : learning rate for our algorithm.\n",
        "### **x_optimum** : will be our current best guess after applying gradient descent with momentum.\n",
        "### **max_iter** : the maximum number of iterations our algorithm will run if not converged.\n",
        "### **part_deri** : partial derivative of delta w.r.t to alpha\n",
        "### **beta** : Hypergradient learning rate\n",
        "&nbsp;\n",
        "### **Return:**\n",
        "\n",
        "### **value** : new optimized value of x\n",
        "### **no. of iterations** : number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "9YBsGSIHM4j5",
        "outputId": "d7a311a0-14c0-4196-98c5-13297e6442f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-9c5ffa9bf8eb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    function HD(x_guess,learn_rate,conv_threshold,max_iter,beta)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "function HD(x_guess,learn_rate,conv_threshold,max_iter,beta)\n",
        "    converged=false\n",
        "    iterations=0\n",
        "    part_deri=0\n",
        "    while converged == false\n",
        "        fd=2*x_guess-2\n",
        "        gamma=fd*part_deri  #<--HyperGradient\n",
        "        learn_rate=learn_rate-beta*gamma    #<--Learning rate update\n",
        "\n",
        "        delta=learn_rate*fd   #parameter update\n",
        "        part_deri=fd      #partial_derivative w.r.t to alpha\n",
        "        x_optimum=x_guess+delta\n",
        "\n",
        "        x_guess=x_optimum\n",
        "        \n",
        "        println(\"no.of iterations: $iterations \",\"value :$x_guess \")\n",
        "\n",
        "        if x_guess <= conv_threshold\n",
        "            converged=true\n",
        "            end\n",
        "        iterations += 1\n",
        "        if iterations > max_iter\n",
        "            converged=true\n",
        "            end\n",
        "        end\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l0GDlT_dWK9",
        "outputId": "9b5c86d8-c651-4b9f-c662-8d4160fa4fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00010000000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQKG4b3PN8eA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5654765b-3b40-4fcd-d73b-383140b107f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no.of iterations: 0 value :10.18 \n",
            "no.of iterations: 1 value :9.75683872 \n",
            "no.of iterations: 2 value :8.790029226603618 \n",
            "no.of iterations: 3 value :7.50483762706951 \n",
            "no.of iterations: 4 value :6.167980652737281 \n",
            "no.of iterations: 5 value :4.966885889903807 \n",
            "no.of iterations: 6 value :3.9798791445276587 \n",
            "no.of iterations: 7 value :3.2102712663348796 \n",
            "no.of iterations: 8 value :2.627782511529386 \n",
            "no.of iterations: 9 value :2.1941160244230344 \n",
            "no.of iterations: 10 value :1.8741281489890644 \n"
          ]
        }
      ],
      "source": [
        "HD(10,0.01,1.0,10,0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use of Hyper gradient descent**\n",
        "\n",
        "1)hyper gradient relies only on the value of beta-hypergradient learning rate and is less sensitive to the initial learning rate.\n",
        "\n",
        "2)Hyper gradient only requires storing of the value for partial derivative w.r.t to learning rate.\n",
        "\n",
        "3)hyper gradient is useful alternative in comparison to adagrad,rmsprop,adam which could face problems with complexity, applicability in practice, or performance."
      ],
      "metadata": {
        "id": "IfrZQPJDIazg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMidUQB03vJ"
      },
      "source": [
        "Add new code cells by clicking the `+ Code` button (or _Insert_ > _Code cell_).\n",
        "\n",
        "Have fun!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-mask.png\" height=\"100\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x2gDaxNj1PL"
      },
      "source": [
        "https://arxiv.org/pdf/1703.04782v1.pdf?source=post_page---------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}